{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection and Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.6.5 (default, Apr  1 2018, 05:46:30) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!which python\n",
    "import sys\n",
    "sys.version\n",
    "\n",
    "# !python3 -c \"import tensorflow as tf; print(tf.__version__)\"\n",
    "# !pip3 install opencv-python\n",
    "# !pip3 install sklearn\n",
    "# !pip3 install keras\n",
    "\n",
    "# !python3 -m pip install Pillow\n",
    "# from keras.preprocessing import image\n",
    "# from PIL import Image\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# from IPython.display import display \n",
    "# from PIL import Image as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Loading Libraries\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224\n",
    "model = VGG16(weights = \"imagenet\" , include_top = True , input_shape=(img_width ,img_height ,3) )\n",
    "# img_path = \"/home/Downloads/demo.jpg\"\n",
    "img_path = \"lfw_smaller/Aaron_Sorkin/Aaron_Sorkin_0001.jpg\"\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "print(x.shape)\n",
    "result = model.predict(x)\n",
    "print(model.summary())\n",
    "img_input = Input(shape=(224, 224, 3))\n",
    "\n",
    "#print decode_predictions(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import decode_predictions\n",
    "# print(decode_predictions(result))\n",
    "print('Predicted:', decode_predictions(result, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing VCG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(input_tensor=img_input, include_top=True,weights='imagenet')\n",
    "last_layer = model.get_layer(\"fc2\").output\n",
    "out = Dense(units = 135, activation='softmax', name='output')(last_layer)\n",
    "# print out \n",
    "custom_vgg_model = Model(img_input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in custom_vgg_model.layers[:-1]:\n",
    "    i.trainable = False\n",
    "custom_vgg_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = train_datagen.flow_from_directory('/home/train_data',\n",
    "training_set = train_datagen.flow_from_directory('lfw_smaller',\n",
    "                                                 target_size = (224, 224),\n",
    "#                                                  target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "print(training_set.image_shape)\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory('/home/train_data',\n",
    "test_set = test_datagen.flow_from_directory('lfw_smaller',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "print(test_set.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "custom_vgg_model.fit_generator(generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, \n",
    "validation_data=None, validation_steps=None, class_weight=None, max_queue_size=10, workers=1, \n",
    "use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
    "'''\n",
    "custom_vgg_model.fit_generator(training_set,\n",
    "                               steps_per_epoch=None, epochs=1, verbose=1,\n",
    "                               validation_data=None, validation_steps=None)\n",
    "#                          steps_per_epoch = 4154,\n",
    "#                          epochs = 25,\n",
    "#                          validation_data = test_set,\n",
    "#                          validation_steps = 4154\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_vgg_model.predict(img_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------Aman Code End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG & CNN based  Face Detection\n",
    "https://www.arunponnusamy.com/cnn-face-detector-dlib.html\n",
    "\n",
    "A Pre Trained Model is used with CNN Detector\n",
    "http://dlib.net/cnn_face_detector.py.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Based On Static image based Detection\n",
    "'''\n",
    "\n",
    "#https://askubuntu.com/questions/824834/how-to-install-dlib-for-python3-in-ubuntu-14-04\n",
    "# !pip3 install dlib\n",
    "\n",
    "# import required packages\n",
    "import cv2\n",
    "import dlib\n",
    "import argparse\n",
    "import time\n",
    "# pip install opencv-python dlib argparse time\n",
    "\n",
    "# handle command line arguments\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('-i', '--image', required=True, help='path to image file')\n",
    "# ap.add_argument('-w', '--weights', default='./mmod_human_face_detector.dat', help='path to weights file')\n",
    "\n",
    "# ap.add_argument('-i', '--image', required=True, help='path to image file')\n",
    "# ap.add_argument('-w', '--weights', default='./mmod_human_face_detector.dat', help='path to weights file')\n",
    "# args = ap.parse_args()\n",
    "\n",
    "image_path = 'training-data/Aaron_Sorkin/Aaron_Sorkin_0001.jpg'\n",
    "weight_path = 'mmod_human_face_detector.dat'\n",
    "\n",
    "\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "# load input image\n",
    "# image = cv2.imread(args.image)\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "if image is None:\n",
    "    print(\"Could not read input image\")\n",
    "    exit()\n",
    "    \n",
    "# initialize hog + svm based face detector\n",
    "hog_face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# initialize cnn based face detector with the weights\n",
    "# cnn_face_detector = dlib.cnn_face_detection_model_v1(args.weights)\n",
    "cnn_face_detector = dlib.cnn_face_detection_model_v1(weight_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Applying HOG face detection\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# apply face detection (hog)\n",
    "faces_hog = hog_face_detector(image, 1)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution Time (in seconds) :\")\n",
    "print(\"HOG : \", format(end - start, '.2f'))\n",
    "\n",
    "# loop over detected faces\n",
    "for face in faces_hog:\n",
    "    x = face.left()\n",
    "    y = face.top()\n",
    "    w = face.right() - x\n",
    "    h = face.bottom() - y\n",
    "\n",
    "    # draw box over face\n",
    "    cv2.rectangle(image, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "\n",
    "\n",
    "    \n",
    "#### Applying CNN face detection\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# apply face detection (cnn)\n",
    "faces_cnn = cnn_face_detector(image, 1)\n",
    "\n",
    "end = time.time()\n",
    "print(\"CNN : \", format(end - start, '.2f'))\n",
    "\n",
    "# loop over detected faces\n",
    "for face in faces_cnn:\n",
    "    x = face.rect.left()\n",
    "    y = face.rect.top()\n",
    "    w = face.rect.right() - x\n",
    "    h = face.rect.bottom() - y\n",
    "\n",
    "     # draw box over face\n",
    "    cv2.rectangle(image, (x,y), (x+w,y+h), (0,0,255), 2)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#### Display results\n",
    "\n",
    "# write at the top left corner of the image\n",
    "# for color identification\n",
    "img_height, img_width = image.shape[:2]\n",
    "cv2.putText(image, \"HOG\", (img_width-50,20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                (0,255,0), 2)\n",
    "cv2.putText(image, \"CNN\", (img_width-50,40), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                (0,0,255), 2)\n",
    "\n",
    "# display output image\n",
    "cv2.imshow(\"face detection with dlib\", image)\n",
    "cv2.waitKey()\n",
    "\n",
    "# save output image \n",
    "cv2.imwrite(\"cnn_face_detection.png\", image)\n",
    "\n",
    "# close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Haar Cascade Based  Face  Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face RecognitionPlus\n",
    "\n",
    "import cv2\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml') # We load the cascade for the face.\n",
    "eye_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_eye.xml')\n",
    "glass_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_eye_tree_eyeglasses.xml')\n",
    "fullbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_fullbody.xml')\n",
    "upperbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_upperbody.xml')\n",
    "lowerbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_lowerbody.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_smile.xml')\n",
    "hand_cascade = cv2.CascadeClassifier('HaarCascade/hand.xml')\n",
    "fist_cascade = cv2.CascadeClassifier('HaarCascade/fist.xml')\n",
    "hand1_cascade = cv2.CascadeClassifier('HaarCascade/Hand.Cascade.1.xml')\n",
    "palm_cascade = cv2.CascadeClassifier('HaarCascade/palm.xml')\n",
    "\n",
    "\n",
    "def detect(gray, frame):\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors= 5)\n",
    "    '''\n",
    "    minSize=(30, 30),\n",
    "    #flags=cv2.cv.CV_HAAR_SCALE_IMAGE\n",
    "    '''\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        cv2.putText(frame, 'This Face!', (x, y), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.2, 4)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.putText(frame, 'eye!', (x+ex, y+ey), font, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "#             cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "            cv2.circle(roi_color,(int(ex + ew/2), int(ey+eh/2)), int(ew/2), (0, 255, 0), 2)\n",
    "#         glass = glass_cascade.detectMultiScale(roi_gray, 1.2, 3)\n",
    "#         for (ex, ey, ew, eh) in glass:\n",
    "#             cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 1.5)\n",
    "        smile = smile_cascade.detectMultiScale(roi_gray, 1.2, 25)\n",
    "        for (sx, sy, sw, sh) in smile:\n",
    "            cv2.putText(frame, 'lips!', (x+sx, y+sy), font, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#             cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 0, 255), 1)\n",
    "#             cv2.circle(roi_color,(int(sx + sw/2), int(sy+sh/2)), int(sw/2), (0, 0, 255), 1)\n",
    "            cv2.ellipse(roi_color,(int(sx + sw/2), int(sy+sh/2)),(int(sw/2),int(sh/2)), 0, 0, 360, (0, 0, 255), 1)\n",
    "#     upbody = upperbody_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#     for (ux, uy, uw, uh) in upbody:\n",
    "#         cv2.rectangle(frame, (ux, uy), (ux+uw, uy+uh), (255, 0, 5), 3)\n",
    "    \n",
    "#     font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#     palm = palm_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#     for (px, py, pw, ph) in palm:\n",
    "#         cv2.putText(frame, 'palm!', (px, py), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "#         cv2.rectangle(frame, (px, py), (px+pw, py+ph), (255, 0, 5), 3)\n",
    "    \n",
    "#     hand = fist_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#     for (hx, hy, hw, hh) in hand:\n",
    "#         cv2.putText(frame, 'hand!', (hx, hy), font, 0.8, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "#         cv2.rectangle(frame, (hx, hy), (hx+hw, hy+hh), (255, 0, 255), 3)\n",
    "            \n",
    "    return frame # We return the image with the detector rectangles.\n",
    "\n",
    "video_capture = cv2.VideoCapture(0) # We turn the webcam on.\n",
    "\n",
    "while True: # We repeat infinitely (until break):\n",
    "    _, frame = video_capture.read() # We get the last frame.\n",
    "#     print(frame)\n",
    "    frame = cv2.flip(frame,1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # We do some colour transformations.\n",
    "    canvas = detect(gray, frame) # We get the output of our detect function.\n",
    "    cv2.imshow('Video', canvas) # We display the outputs.\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "        break # We stop the loop.\n",
    "\n",
    "video_capture.release() # We turn the webcam off.\n",
    "cv2.destroyAllWindows() # We destroy all the windows inside which the images were displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 105) (171, 179)\n",
      "(96, 105) (171, 179)\n",
      "(96, 105) (171, 179)\n",
      "(96, 105) (171, 179)\n",
      "(104, 88) (179, 163)\n",
      "(121, 88) (196, 163)\n",
      "(128, 94) (190, 156)\n",
      "(128, 94) (190, 156)\n",
      "(128, 94) (190, 156)\n",
      "(128, 94) (190, 156)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Based On Real image based Face Detection\n",
    "'''\n",
    "\n",
    "import cv2, time, numpy as np\n",
    "import dlib\n",
    "\n",
    "weight_path = 'mmod_human_face_detector.dat'\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml') # We load the cascade for the face.\n",
    "eye_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_eye.xml')\n",
    "glass_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_eye_tree_eyeglasses.xml')\n",
    "fullbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_fullbody.xml')\n",
    "upperbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_upperbody.xml')\n",
    "lowerbody_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_lowerbody.xml')\n",
    "smile_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_smile.xml')\n",
    "hand_cascade = cv2.CascadeClassifier('HaarCascade/hand.xml')\n",
    "fist_cascade = cv2.CascadeClassifier('HaarCascade/fist.xml')\n",
    "hand1_cascade = cv2.CascadeClassifier('HaarCascade/Hand.Cascade.1.xml')\n",
    "palm_cascade = cv2.CascadeClassifier('HaarCascade/palm.xml')\n",
    "\n",
    "\n",
    "\n",
    "def detect_Haar(gray, frame, SaveFace = False):\n",
    "    \n",
    "    # DetectedFace = np.zeros((gray.shape[1], gray.shape[0],3), np.uint8) \n",
    "    faceimageshape = (280, 320)\n",
    "    DetectedFace = np.zeros((faceimageshape[0], faceimageshape[1]), np.uint8) ## Creating blank image\n",
    "    \n",
    "    start = time.time()\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors= 5)\n",
    "    end = time.time()\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.putText(frame, text = 'Haar Detector' +  format(end - start, '.2f') + 's', \n",
    "                    org =(x, y), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.4, \n",
    "                    color = (255, 0, 0), thickness = 2, lineType = cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray, 1.2, 4)\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            cv2.putText(frame, text = 'Haar eye!', org= (x+ex, y+ey), fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        fontScale = 0.25, color = (0, 255, 0), thickness = 1, lineType = cv2.LINE_AA)\n",
    "            # cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 2)\n",
    "            cv2.circle(roi_color,(int(ex + ew/2), int(ey+eh/2)), int(ew/2), (0, 255, 0), 2)\n",
    "        # glass = glass_cascade.detectMultiScale(roi_gray, 1.2, 3)\n",
    "        # for (ex, ey, ew, eh) in glass:\n",
    "        #     cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 255, 0), 1.5)\n",
    "        smile = smile_cascade.detectMultiScale(roi_gray, 1.2, 25)\n",
    "        for (sx, sy, sw, sh) in smile:\n",
    "            cv2.putText(frame, text = 'Haar lips!', org= (x+sx, y+sy), fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        fontScale = 0.25, color = (0, 0, 255), thickness = 1, lineType = cv2.LINE_AA)\n",
    "            # cv2.rectangle(roi_color,(ex, ey),(ex+ew, ey+eh), (0, 0, 255), 1)\n",
    "            # cv2.circle(roi_color,(int(sx + sw/2), int(sy+sh/2)), int(sw/2), (0, 0, 255), 1)\n",
    "            cv2.ellipse(roi_color,(int(sx + sw/2), int(sy+sh/2)),(int(sw/2),int(sh/2)), 0, 0, 360, (0, 0, 255), 1)\n",
    "    # upbody = upperbody_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    # for (ux, uy, uw, uh) in upbody:\n",
    "    #     cv2.rectangle(frame, (ux, uy), (ux+uw, uy+uh), (255, 0, 5), 3)\n",
    "    # font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # palm = palm_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    # for (px, py, pw, ph) in palm:\n",
    "    #     cv2.putText(frame, 'palm!', (px, py), font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    #     cv2.rectangle(frame, (px, py), (px+pw, py+ph), (255, 0, 5), 3)\n",
    "    # hand = fist_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    # for (hx, hy, hw, hh) in hand:\n",
    "    #     cv2.putText(frame, 'hand!', (hx, hy), font, 0.8, (255, 0, 255), 2, cv2.LINE_AA)\n",
    "    #     cv2.rectangle(frame, (hx, hy), (hx+hw, hy+hh), (255, 0, 255), 3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(faces) > 0:\n",
    "        #cv2.imwrite('saveFace' + str(time.time()) + '.jpg',frame[x:x+w, y:y+h])\n",
    "        DetectedFace = cv2.resize(gray[x:x+w, y:y+h], dsize = faceimageshape) \n",
    "        # print(DetectedFace)\n",
    "        if SaveFace: cv2.imwrite(filename = 'WebcamExtractedFaces/SaveFace_Haar_' + str(int(time.time())) + '.jpg', img = DetectedFace) ## int time to have less number of images ##smaller image is saved \n",
    "        # break\n",
    "    \n",
    "    return frame, DetectedFace ## Colored image & subset grey cropped image\n",
    "\n",
    "\n",
    "def detect_HOG(gray, frame, SaveFace = False):\n",
    "    faceimageshape = (280, 320)\n",
    "    DetectedFace = np.zeros((faceimageshape[1], faceimageshape[0]), np.uint8)\n",
    "    start = time.time()\n",
    "    # initialize hog + svm based face detector\n",
    "    hog_face_detector = dlib.get_frontal_face_detector()\n",
    "    faces = hog_face_detector(gray, 1)\n",
    "    end = time.time()\n",
    "    for face in faces:\n",
    "        x = face.left()\n",
    "        y = face.top()\n",
    "        w = face.right() - x\n",
    "        h = face.bottom() - y\n",
    "        cv2.putText(frame, text = 'HOG Detector' +  format(end - start, '.2f') + 's', \n",
    "                    org =(x, y), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.5, \n",
    "                    color = (0, 0, 255), thickness = 2, lineType = cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, pt1 = (x,y), pt2 = (x+w,y+h), color = (0,0,255), \n",
    "                      thickness = 2, lineType = cv2.LINE_AA)\n",
    "        print((x,y), (x+w,y+h))\n",
    "        if len(faces) > 0:\n",
    "            print(\"WWWWW\", (x,x+w), '__', (y,y+h))\n",
    "            DetectedFace = cv2.resize(gray[int(x):int(x+w), int(y):int(y+h)], dsize = faceimageshape) \n",
    "            if SaveFace: cv2.imwrite(filename = 'WebcamExtractedFaces/SaveFace_HOG_' + str(int(time.time())) + '.jpg', img = DetectedFace) ## int time to have less number of images ##smaller image is saved \n",
    "            # break\n",
    "    return frame, DetectedFace\n",
    "\n",
    "\n",
    "def detect_CNN(gray, frame, SaveFace = False):\n",
    "    faceimageshape = (280, 320)\n",
    "    DetectedFace = np.zeros((faceimageshape[0], faceimageshape[1]), np.uint8)\n",
    "    start = time.time()\n",
    "    # initialize cnn based face detector with the weights\n",
    "    cnn_face_detector = dlib.cnn_face_detection_model_v1(weight_path)\n",
    "    faces = cnn_face_detector(gray, 1)\n",
    "    end = time.time()\n",
    "    for face in faces:\n",
    "        x = face.rect.left()\n",
    "        y = face.rect.top()\n",
    "        w = face.rect.right() - x\n",
    "        h = face.rect.bottom() - y\n",
    "        \n",
    "        cv2.putText(frame, text = 'CNN Detector' +  format(end - start, '.2f') + 's', \n",
    "                    org =(x, y), fontFace = cv2.FONT_HERSHEY_SIMPLEX, fontScale = 0.5, \n",
    "                    color = (0, 255, 0), thickness = 2, lineType = cv2.LINE_AA)\n",
    "        cv2.rectangle(frame, pt1 = (x,y), pt2 = (x+w,y+h), color = (0,255,0), \n",
    "                      thickness = 2, lineType = cv2.LINE_AA)\n",
    "    if len(faces) > 0:\n",
    "        DetectedFace = cv2.resize(gray[x:x+w, y:y+h], dsize = faceimageshape) \n",
    "        if SaveFace: cv2.imwrite(filename = 'WebcamExtractedFaces/SaveFace_CNN_' + str(int(time.time())) + '.jpg', img = DetectedFace) ## int time to have less number of images ##smaller image is saved \n",
    "        # break\n",
    "    return frame, DetectedFace\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Haar Cascade ----Mixing is not Done because of the latency\n",
    "# video_capture = cv2.VideoCapture(0) # We turn the webcam on.\n",
    "# if(not video_capture.isOpened()):\n",
    "#     print('Error Camera')\n",
    "# saveFace = None\n",
    "# while True: # We repeat infinitely (until break):\n",
    "# #     _, frame = video_capture.read() # We get the last frame.\n",
    "# #     print(frame)\n",
    "#     retval, frame = video_capture.read() # We get the last frame.\n",
    "#     if retval == False:\n",
    "#         break # Exit if video has ended\n",
    "#     elif retval ==True:\n",
    "        \n",
    "# #         mask = np.zeros_like(frame)  # init mask\n",
    "# #         contours = find_contours(frame)\n",
    "# #         plates, plates_images, mask = find_plate_numbers(frame, contours, mask)\n",
    "# #         processed_frame = cv2.add(frame, mask)  # Apply the mask to image\n",
    "# #         cv2.imshow('frame', processed_frame)\n",
    "        \n",
    "        \n",
    "#         ## To Pace Thing up -- almost real time\n",
    "#         frame = cv2.resize(frame, dsize=None, fx=0.5, fy=0.5) \n",
    "#         frame = cv2.flip(frame, 1)\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # We do some colour transformations.\n",
    "        \n",
    "#         # print('Frame Shape:',frame.shape) ## Colored Image\n",
    "#         # print('Gray Shape:', gray.shape)  ## Grey Image\n",
    "        \n",
    "#         canvas, grey_face_canvas = detect_Haar(gray, frame, SaveFace = True)\n",
    "        \n",
    "#         canvas = cv2.resize(canvas, dsize=None, fx=3, fy=3) ## To increase Size Again\n",
    "#         cv2.imshow('Video: Haar Cascade Based Face  Detection', canvas)\n",
    "#         cv2.imshow('Video: Haar Cascade Based Detected  Face', grey_face_canvas)\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'): # If we type on the keyboard:\n",
    "#             video_capture.release() # We turn the webcam off.\n",
    "#             cv2.destroyAllWindows() # We destroy all the windows inside which the images were displayed.\n",
    "#             break # We stop the loop.\n",
    "\n",
    "\n",
    "\n",
    "## Haar/HOG/CNN based Face Detection\n",
    "WhichDetectorToUse = 'HOG'\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "if(not video_capture.isOpened()):\n",
    "    print('Error Camera')\n",
    "while True:\n",
    "    retval, frame = video_capture.read()\n",
    "    if retval == False:\n",
    "        break\n",
    "    elif retval ==True:\n",
    "        frame = cv2.resize(frame, dsize=None, fx=0.5, fy=0.5) \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        if WhichDetectorToUse == 'Haar':\n",
    "            canvas, grey_face_canvas = detect_Haar(gray, frame, SaveFace = True)\n",
    "        elif WhichDetectorToUse == 'HOG':\n",
    "            canvas, grey_face_canvas = detect_HOG(gray, frame, SaveFace = True)\n",
    "        elif WhichDetectorToUse == 'CNN':\n",
    "            canvas, grey_face_canvas = detect_CNN(gray, frame, SaveFace = True)\n",
    "        canvas = cv2.resize(canvas, dsize=None, fx=3, fy=3)\n",
    "        cv2.imshow('Video: HOG Based Face Detection', canvas)\n",
    "        cv2.imshow('Video: HOG Based Detected Face', grey_face_canvas)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            video_capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "\n",
    "        \n",
    "# ## Mixed All In Single Frame\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     _, frame = video_capture.read()\n",
    "#     frame = cv2.flip(frame, 1)\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     canvas = detect_Haar(gray, frame)\n",
    "#     canvas = detect_HOG(gray, canvas)\n",
    "#     canvas = detect_CNN(gray, canvas)\n",
    "#     cv2.imshow('Video: Haar Cascade, HOG, &  CNN Based Face Detection', canvas)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Motion Sensor\n",
    "# def video (seconds, frameRate):\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#     if(not cap.isOpened()):\n",
    "#         return \"error\"\n",
    "\n",
    "#     # Define the codec and create VideoWriter object\n",
    "#     fourcc = cv2.cv.CV_FOURCC(*'XVID')\n",
    "#     name = \"media/video/\" + time.strftime(\"%d-%m-%Y_%X\")+\".avi\"\n",
    "#     out = cv2.VideoWriter(name, fourcc, frameRate, (640,480))\n",
    "#     program_starts = time.time()\n",
    "#     result = subprocess.Popen([\"ffprobe\", name], stdout = subprocess.PIPE, stderr = subprocess.STDOUT, shell=True)\n",
    "#     nFrames=0\n",
    "#     while(nFrames<seconds*frameRate):\n",
    "#         ret, frame = cap.read()\n",
    "#         if ret==True:\n",
    "#             out.write(frame)\n",
    "#             nFrames += 1\n",
    "#         else:\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     return name \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Motion Detection:\n",
    "https://github.com/RobinDavid/Motion-detection-OpenCV/blob/master/MotionDetector.py :::: Too Many Error To Resolve -----> HEnce Ignnoring\n",
    "https://www.pyimagesearch.com/2015/05/25/basic-motion-detection-and-tracking-with-python-and-opencv/ ------>>>>>  Implemented\n",
    "http://www.steinm.com/blog/motion-detection-webcam-python-opencv-differential-images/     ---------->>>> Impplemented\n",
    "'''\n",
    "import cv2\n",
    "\n",
    "def differentialImage(t0, t1, t2):\n",
    "    d1 = cv2.absdiff(t2, t1)\n",
    "    d2 = cv2.absdiff(t1, t0)\n",
    "    return cv2.bitwise_and(d1, d2)\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "winName = \"Movement Indicator\"\n",
    "cv2.namedWindow(winName, cv2.WINDOW_AUTOSIZE) #,cv2.WINDOW_NORMAL\n",
    "\n",
    "# Read three images first:\n",
    "t_minus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t_plus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "while True:\n",
    "    cv2.imshow(winName, differentialImage(t_minus, t, t_plus))\n",
    "    #cv2.imwrite(\"image.png\", diffImg(t_minus, t, t_plus))\n",
    "\n",
    "    # Read next image\n",
    "    t_minus = t\n",
    "    t = t_plus\n",
    "    t_plus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "\n",
    "print('Camera Turning Off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Motion Detection ---+ mask + Face Detection Integration (add +20% more area around Face)\n",
    "'''\n",
    "\n",
    "import cv2, numpy as np, sys\n",
    "\n",
    "def differentialImage(t0, t1, t2):\n",
    "    \n",
    "    # compute the absolute difference between the current frame and\n",
    "    frameDelta_d1 = cv2.absdiff(t2, t1)\n",
    "    thresh_d1 = cv2.threshold(frameDelta_d1, 20, 255, cv2.THRESH_BINARY)[1]\n",
    "    frameDelta_d2 = cv2.absdiff(t1, t0)\n",
    "    thresh_d2 = cv2.threshold(frameDelta_d2, 20, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # dilate the thresholded image to fill in holes, then find contours on thresholded image\n",
    "    # Taking a matrix of size 5 as the kernel\n",
    "    kernel = np.ones((1,1), np.uint8)\n",
    "    \n",
    "    thresh_d1 = cv2.dilate(thresh_d1, kernel, iterations=2)\n",
    "    cnts1 = cv2.findContours(thresh_d1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #cnts1 = cnts1[0] if imutils.is_cv2() else cnts1[1]\n",
    "    thresh_d2 = cv2.dilate(thresh_d2, kernel, iterations=2)\n",
    "    cnts2 = cv2.findContours(thresh_d2.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    #cnts2 = cnts2[0] if imutils.is_cv2() else cnts2[1]\n",
    "    \n",
    "#     return cv2.bitwise_and(frameDelta_d1, frameDelta_d2)\n",
    "    return cv2.bitwise_or(thresh_d1, thresh_d2)\n",
    "#     return cv2.bitwise_or(cnts1, cnts2)\n",
    "\n",
    "\n",
    "\n",
    "print('Camera Turning On')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "if(not video_capture.isOpened()):\n",
    "    print('Error Camera')\n",
    "    sys.exit(1)\n",
    "\n",
    "# Read three images first:\n",
    "t_minus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "t_plus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# t_minus = cv2.GaussianBlur(t_minus, (21, 21), 0)\n",
    "# t = cv2.GaussianBlur(t, (21, 21), 0)\n",
    "# t_plus = cv2.GaussianBlur(t_plus, (21, 21), 0)\n",
    "    \n",
    "while True:\n",
    "    retval, frame = video_capture.read()\n",
    "    if retval == False:\n",
    "        break\n",
    "    elif retval ==True:\n",
    "        \n",
    "        \n",
    "        cv2.imshow('Movement Indicator', differentialImage(t_minus, t, t_plus))\n",
    "        #cv2.imwrite(\"image.png\", diffImg(t_minus, t, t_plus))\n",
    "        ## Read next image\n",
    "        t_minus = t\n",
    "        t = t_plus\n",
    "        t_plus = cv2.cvtColor(video_capture.read()[1], cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "#         t_minus = cv2.GaussianBlur(t_minus, (21, 21), 0)\n",
    "#         t = cv2.GaussianBlur(t, (21, 21), 0)\n",
    "#         t_plus = cv2.GaussianBlur(t_plus, (21, 21), 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#         frame = cv2.resize(frame, dsize=None, fx=0.5, fy=0.5) \n",
    "#         frame = cv2.flip(frame, 1)\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         if WhichDetectorToUse == 'Haar':\n",
    "#             canvas, grey_face_canvas = detect_Haar(gray, frame, SaveFace = True)\n",
    "#         elif WhichDetectorToUse == 'HOG':\n",
    "#             canvas, grey_face_canvas = detect_HOG(gray, frame, SaveFace = True)\n",
    "#         elif WhichDetectorToUse == 'CNN':\n",
    "#             canvas, grey_face_canvas = detect_CNN(gray, frame, SaveFace = True)\n",
    "#         frame = cv2.resize(frame, dsize=None, fx=3, fy=3)\n",
    "#         cv2.imshow('Video: HOG Based Face Detection', canvas)\n",
    "#         cv2.imshow('Video: HOG Based Detected Face', grey_face_canvas)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print('Camera Turning Off')\n",
    "            video_capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCv Based Face Detection and Recognition\n",
    "https://www.hackster.io/mjrobot/real-time-face-recognition-an-end-to-end-project-a10826  \n",
    "![Path](FaceRecogBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # set Width\n",
    "cap.set(4,480) # set Height\n",
    " \n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "#     frame = cv2.flip(frame, -1) # Flip camera vertically\n",
    "    frame = cv2.flip(frame, 1) # Flip camera horizontally\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('gray', gray)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# faceCascade = cv2.CascadeClassifier('Cascades/haarcascade_frontalface_default.xml')\n",
    "face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3,640) # set Width\n",
    "cap.set(4,480) # set Height\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors= 5, minSize=(20, 20))   \n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]  \n",
    "\n",
    "    cv2.imshow('video',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27: # press 'ESC' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Enter user First name: aakanksha\n",
      "\n",
      " Enter user Last name: asd\n",
      "\n",
      "Adding facial Information of a new person\n",
      "\n",
      " [INFO] Initializing face capture. Look at the camera and wait ...\n",
      "(166, 338)\n",
      "Image 1 Taken\n",
      "(194, 335)\n",
      "Image 2 Taken\n",
      "(194, 327)\n",
      "Image 3 Taken\n",
      "(171, 183)\n",
      "Image 4 Taken\n",
      "(169, 310)\n",
      "Image 5 Taken\n",
      "(152, 282)\n",
      "Image 6 Taken\n",
      "(146, 281)\n",
      "Image 7 Taken\n",
      "(90, 284)\n",
      "Image 8 Taken\n",
      "(66, 281)\n",
      "Image 9 Taken\n",
      "(143, 282)\n",
      "Image 10 Taken\n",
      "(126, 275)\n",
      "Image 11 Taken\n",
      "(128, 253)\n",
      "Image 12 Taken\n",
      "(114, 225)\n",
      "Image 13 Taken\n",
      "(90, 200)\n",
      "Image 14 Taken\n",
      "(98, 186)\n",
      "Image 15 Taken\n",
      "(120, 137)\n",
      "Image 16 Taken\n",
      "(125, 120)\n",
      "Image 17 Taken\n",
      "(186, 90)\n",
      "Image 18 Taken\n",
      "(198, 78)\n",
      "Image 19 Taken\n",
      "(202, 87)\n",
      "Image 20 Taken\n",
      "(204, 87)\n",
      "Image 21 Taken\n",
      "(205, 96)\n",
      "Image 22 Taken\n",
      "(212, 107)\n",
      "Image 23 Taken\n",
      "(210, 110)\n",
      "Image 24 Taken\n",
      "(216, 121)\n",
      "Image 25 Taken\n",
      "(211, 124)\n",
      "Image 26 Taken\n",
      "(217, 131)\n",
      "Image 27 Taken\n",
      "(221, 137)\n",
      "Image 28 Taken\n",
      "(225, 147)\n",
      "Image 29 Taken\n",
      "(223, 148)\n",
      "Image 30 Taken\n",
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>PersonName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mohit_Rajput_id0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>aman_rawka_id0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mohit__id0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>aakanksha_asd_id0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID         PersonName\n",
       "0   1   Mohit_Rajput_id0\n",
       "1   2     aman_rawka_id0\n",
       "2   3         Mohit__id0\n",
       "3   4  aakanksha_asd_id0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import os,  sys\n",
    "import pandas as pd\n",
    "import glob, time\n",
    "\n",
    "\n",
    "def CapturePersonFaceImages(dirToSaveImageIn, perFaceId):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cam = cv2.VideoCapture(0)\n",
    "    cam.set(3, 640) # set video width\n",
    "    cam.set(4, 480) # set video height\n",
    "    \n",
    "    # face_detector = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    print(\"\\n [INFO] Initializing face capture. Look at the camera and wait ...\")\n",
    "    # Initialize individual sampling face count\n",
    "    AllFiles = glob.glob(dirToSaveImageIn + 'User.{}.*.jpg'.format(perFaceId))\n",
    "    ini_count = len(AllFiles) #0 if len(AllFiles) == 0 else 1\n",
    "    count = 0\n",
    "    \n",
    "    while 0xFF == 255:\n",
    "        ret, img = cam.read()\n",
    "        cv2.putText(img, text = 'Press any button when ready!', org= (15, 60), fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale = 1, color = (0, 0, 255), thickness = 1, lineType = cv2.LINE_AA)\n",
    "        cv2.imshow('image', img)\n",
    "        time.sleep(0.1)\n",
    "        if  cv2.waitKey(100) & 0xff != 255:\n",
    "            break\n",
    "    \n",
    "    while(True):\n",
    "        ret, img = cam.read()\n",
    "        img = cv2.flip(img, 1)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        cv2.putText(img, text = 'Please look at the camera!', org= (15, 30), fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale = 1, color = (0, 0, 255), thickness = 1, lineType = cv2.LINE_AA)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors= 5, minSize=(20, 20))\n",
    "        for (x,y,w,h) in faces:\n",
    "            print((x,y))\n",
    "            cv2.rectangle(img, (x,y), (x+w,y+h), (255,0,0), 2)\n",
    "            cv2.imshow('image', img) ## frame  stops if  face is not visbile\n",
    "        cv2.imshow('image', img) ## frame keep on showing\n",
    "        if len(faces) > 0:\n",
    "            # DetectedFace = cv2.resize(gray[x:x+w, y:y+h], dsize = faceimageshape) \n",
    "            count += 1\n",
    "            print('Image {} Taken'.format(str(count)))\n",
    "            time.sleep(0.1)\n",
    "            # Save the captured image into the datasets folder\n",
    "            cv2.imwrite(dirToSaveImageIn + 'User.' + str(perFaceId) + '.' + str(ini_count+count) + \".jpg\", gray[y:y+h,x:x+w])\n",
    "        \n",
    "#         if cv2.waitKey(100) & 0xff != 255:\n",
    "            \n",
    "        k = cv2.waitKey(100) & 0xff # Press 'ESC' for exiting video\n",
    "        if k == 27:\n",
    "            break\n",
    "        elif count >= 30: # Take 30 face sample and stop video\n",
    "             break\n",
    "            \n",
    "    # Do a bit of cleanup\n",
    "    print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# Path for face image database\n",
    "path = 'dataset/'\n",
    "personIdMapFile_path = 'MappingFaceToId.csv'\n",
    "\n",
    "\n",
    "# For each person, enter one numeric face id\n",
    "personFirName = input('\\n Enter user First name: ')# Hit Enter<return> at the end. #input('\\n enter user id end press <return> ==>  ')\n",
    "personLasName = input('\\n Enter user Last name: ')\n",
    "personName = '_'.join([personFirName, personLasName, 'id0'])\n",
    "#print(personName)\n",
    "\n",
    "if os.path.exists(personIdMapFile_path):\n",
    "    perMap = pd.read_csv(personIdMapFile_path)\n",
    "    if personName in list(perMap['PersonName']):\n",
    "        ## if person name is already present add more data to it\n",
    "        print('\\nThis Person Nane is already present in the DB')\n",
    "        action = input('Type \"SamePerson\" to add more data to this person profile or \"Exit\" to exit or \"NewPerson\" to add information for a new person having same name')\n",
    "        if action == 'SamePerson':\n",
    "            face_id = perMap.loc[perMap['PersonName']==personName, 'ID'][0]\n",
    "            CapturePersonFaceImages(path, face_id)\n",
    "        elif action == 'NewPerson':\n",
    "            pVal = sum(perMap['PersonName'] == personName) + 1\n",
    "            personName = '_'.join([personName.split('_')[0], personName.split('_')[1], 'id'+str(pVal)])\n",
    "            face_id = perMap.shape[0] + 1\n",
    "            perMap = perMap.append({'ID':face_id, 'PersonName':personName}, ignore_index=True)\n",
    "            CapturePersonFaceImages(path, face_id)\n",
    "        else:\n",
    "            sys.exit('User Selected the exit command, when name of the person was matching.')\n",
    "    else:\n",
    "        ## adding impformation about new person\n",
    "        print('\\nAdding facial Information of a new person')\n",
    "        face_id = max(perMap['ID']) + 1 ## other just get the length of the df \n",
    "        perMap = perMap.append({'ID':face_id, 'PersonName':personName}, ignore_index=True)\n",
    "        CapturePersonFaceImages(path, face_id)\n",
    "else:\n",
    "    ## This is the first iteration creating mmapping dataset\n",
    "    perMap = pd.DataFrame({'ID':1, 'PersonName':personName}, columns=['ID', 'PersonName'],  index=[0])\n",
    "    CapturePersonFaceImages(path, 1)\n",
    "\n",
    "perMap.to_csv(personIdMapFile_path, index=False)\n",
    "perMap.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.waitKey(100) & 0xff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Training faces. It will take a few seconds. Wait ...\n",
      "\n",
      " [INFO] 4 faces trained. Exiting Program\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Path for face image database\n",
    "path = 'dataset'\n",
    "model_Saving_Path = 'trainer'\n",
    "\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()  ## requires to install: pip3 install opencv-contrib-python\n",
    "detector = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# function to get the images and label data\n",
    "def getImagesAndLabels(path):\n",
    "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    faceSamples=[]\n",
    "    ids = []\n",
    "    for imagePath in imagePaths:\n",
    "        PIL_img = Image.open(imagePath).convert('L') # convert it to grayscale\n",
    "        img_numpy = np.array(PIL_img,'uint8')\n",
    "        id = int(os.path.split(imagePath)[-1].split(\".\")[1])\n",
    "#         id = int(os.path.split(imagePath)[-1].split(\".\")[-2])\n",
    "        faces = detector.detectMultiScale(img_numpy)\n",
    "        for (x,y,w,h) in faces:\n",
    "            faceSamples.append(img_numpy[y:y+h,x:x+w])\n",
    "            ids.append(id)\n",
    "    return faceSamples,ids\n",
    "\n",
    "print (\"\\n [INFO] Training faces. It will take a few seconds. Wait ...\")\n",
    "faces,ids = getImagesAndLabels(path)\n",
    "recognizer.train(faces, np.array(ids))\n",
    "\n",
    "# Save the model into trainer/trainer.yml\n",
    "recognizer.write(model_Saving_Path + '/trainer.yml') # recognizer.save() worked on Mac, but not on Pi\n",
    "\n",
    "# Print the numer of faces trained and end program\n",
    "print(\"\\n [INFO] {0} faces trained. Exiting Program\".format(len(np.unique(ids))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import opencv-contrib-python\n",
    "# cv2.FisherFaceRecognizer_create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None', 'Mohit_Rajput_id0', 'aman_rawka_id0']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personIdMapFile_path = 'MappingFaceToId.csv'\n",
    "perDf = pd.read_csv(personIdMapFile_path)\n",
    "['None'] + list(perDf['PersonName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [INFO] Exiting Program and cleanup stuff\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os, pandas as pd\n",
    "\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "## Three Recognizer Available with OpenCV\n",
    "# Eigen Face  Recognizer\n",
    "# Fisher Face  Recognizer\n",
    "# LBPH Face Recognizer\n",
    "\n",
    "personIdMapFile_path = 'MappingFaceToId.csv'\n",
    "perDf = pd.read_csv(personIdMapFile_path)\n",
    "recognizer.read('trainer/trainer.yml')\n",
    "cascadePath = \"HaarCascade/haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascadePath);\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "#iniciate id counter\n",
    "id = 0\n",
    "\n",
    "# names related to ids: example ==> Marcelo: id=1,  etc\n",
    "names = ['None'] + list(perDf['PersonName']) \n",
    "\n",
    "# Initialize and start realtime video capture\n",
    "cam = cv2.VideoCapture(0)\n",
    "cam.set(3, 640) # set video widht\n",
    "cam.set(4, 480) # set video height\n",
    "\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1*cam.get(3)\n",
    "minH = 0.1*cam.get(4)\n",
    "\n",
    "while True:\n",
    "    ret, img =cam.read()\n",
    "    img = cv2.flip(img, 1) # Flip horizontally\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor = 1.2, minNeighbors = 5, minSize = (int(minW), int(minH)))\n",
    "\n",
    "    for(x,y,w,h) in faces:\n",
    "        cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2)\n",
    "        id, confidence = recognizer.predict(gray[y:y+h,x:x+w])\n",
    "\n",
    "        # Check if confidence is less them 100 ==> \"0\" is perfect match \n",
    "        if (confidence < 100):\n",
    "            id = names[id]\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        else:\n",
    "            id = \"unknown\"\n",
    "            confidence = \"  {0}%\".format(round(100 - confidence))\n",
    "        \n",
    "        cv2.putText(img, str(id), (x+5,y-5), font, 1, (255,255,255), 2) ## Displaying Name\n",
    "        cv2.putText(img, str(confidence), (x+5,y+h-5), font, 1, (255,255,0), 1) ## Displaying Confidence\n",
    "    \n",
    "    cv2.imshow('camera',img) \n",
    "\n",
    "    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting video\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Do a bit of cleanup\n",
    "print(\"\\n [INFO] Exiting Program and cleanup stuff\")\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Snippet -- Plain Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = faceCascade.detectMultiScale(\n",
    "        gray,     \n",
    "        scaleFactor=1.2,\n",
    "        minNeighbors=5,     \n",
    "        minSize=(20, 20)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saving a Video\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret==True:\n",
    "        frame = cv2.flip(frame,1)\n",
    "\n",
    "        # write the flipped frame\n",
    "        out.write(frame)\n",
    "\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCV\n",
    "https://www.superdatascience.com/opencv-face-recognition/  \n",
    "https://github.com/informramiz/opencv-face-recognition-python -- >  better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenCV module\n",
    "import cv2\n",
    "#os module for reading training data directories and paths\n",
    "import os\n",
    "#numpy to convert python lists to numpy arrays as it is needed by OpenCV face recognizers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is no label 0 in our training data so subject name for index/label 0 is empty\n",
    "subjects = [\"\", \"Ramiz Raja\", \"Elvis Presley\"]\n",
    "subject = [\"\", \"Aaron_Peirsol\", \"Abdullah_Gul\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation for Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to detect face using OpenCV\n",
    "def detect_face(img):\n",
    "    #convert the test image to gray scale as opencv face detector expects gray images\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #load OpenCV face detector, I am using LBP which is fast\n",
    "    #there is also a more accurate but slow: Haar classifier\n",
    "    # face_cascade = cv2.CascadeClassifier('opencv-files/lbpcascade_frontalface.xml')\n",
    "    face_cascade = cv2.CascadeClassifier('HaarCascade/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    #let's detect multiscale images(some images may be closer to camera than others)\n",
    "    #result is a list of faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);\n",
    "\n",
    "    #if no faces are detected then return original img\n",
    "    if (len(faces) == 0):\n",
    "        return None, None\n",
    "\n",
    "    #under the assumption that there will be only one face,\n",
    "    #extract the face area\n",
    "    (x, y, w, h) = faces[0]\n",
    "\n",
    "    #return only the face part of the image\n",
    "    return gray[y:y+w, x:x+h], faces[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function will read all persons' training images, detect face from each image\n",
    "#and will return two lists of exactly same size, one list \n",
    "#of faces and another list of labels for each face\n",
    "def prepare_training_data(data_folder_path):\n",
    "    #------STEP-1--------\n",
    "    #get the directories (one directory for each subject) in data folder\n",
    "    dirs = os.listdir(data_folder_path)\n",
    "\n",
    "    #list to hold all subject faces\n",
    "    faces = []\n",
    "    #list to hold labels for all subjects\n",
    "    labels = []\n",
    "\n",
    "    #let's go through each directory and read images within it\n",
    "    for dir_name in dirs:\n",
    "\n",
    "        #our subject directories start with letter 's' so\n",
    "        #ignore any non-relevant directories if any\n",
    "        if not dir_name.startswith(\"s\"):\n",
    "            continue;\n",
    "\n",
    "    #------STEP-2--------\n",
    "    #extract label number of subject from dir_name\n",
    "    #format of dir name = slabel\n",
    "    #, so removing letter 's' from dir_name will give us label\n",
    "    label = int(dir_name.replace(\"s\", \"\"))\n",
    "\n",
    "    #build path of directory containing images for current subject subject\n",
    "    #sample subject_dir_path = \"training-data/s1\"\n",
    "    subject_dir_path = data_folder_path + \"/\" + dir_name\n",
    "\n",
    "    #get the images names that are inside the given subject directory\n",
    "    subject_images_names = os.listdir(subject_dir_path)\n",
    "\n",
    "    #------STEP-3--------\n",
    "    #go through each image name, read image, \n",
    "    #detect face and add face to list of faces\n",
    "    for image_name in subject_images_names:\n",
    "\n",
    "        #ignore system files like .DS_Store\n",
    "        if image_name.startswith(\".\"):\n",
    "            continue;\n",
    "\n",
    "    #build image path\n",
    "    #sample image path = training-data/s1/1.pgm\n",
    "    image_path = subject_dir_path + \"/\" + image_name\n",
    "\n",
    "    #read image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    #display an image window to show the image \n",
    "    cv2.imshow(\"Training on image...\", image)\n",
    "    cv2.waitKey(100)\n",
    "\n",
    "    #detect face\n",
    "    face, rect = detect_face(image)\n",
    "\n",
    "    #------STEP-4--------\n",
    "    #for the purpose of this tutorial\n",
    "    #we will ignore faces that are not detected\n",
    "    if face is not None:\n",
    "        #add face to list of faces\n",
    "        faces.append(face)\n",
    "        #add label for this face\n",
    "        labels.append(label)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return faces, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's first prepare our training data\n",
    "#data will be in two lists of same size\n",
    "#one list will contain all the faces\n",
    "#and the other list will contain respective labels for each face\n",
    "print(\"Preparing data...\")\n",
    "faces, labels = prepare_training_data(\"training-data\")\n",
    "print(\"Data prepared\")\n",
    "\n",
    "#print total faces and labels\n",
    "print(\"Total faces: \", len(faces))\n",
    "print(\"Total labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Face Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our LBPH face recognizer \n",
    "face_recognizer = cv2.face.createLBPHFaceRecognizer()\n",
    "\n",
    "#or use EigenFaceRecognizer by replacing above line with \n",
    "#face_recognizer = cv2.face.createEigenFaceRecognizer()\n",
    "\n",
    "#or use FisherFaceRecognizer by replacing above line with \n",
    "#face_recognizer = cv2.face.createFisherFaceRecognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train our face recognizer of our training faces\n",
    "face_recognizer.train(faces, np.array(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to draw rectangle on image \n",
    "#according to given (x, y) coordinates and \n",
    "#given width and heigh\n",
    "def draw_rectangle(img, rect):\n",
    "    (x, y, w, h) = rect\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "#function to draw text on give image starting from\n",
    "#passed (x, y) coordinates. \n",
    "def draw_text(img, text, x, y):\n",
    "    cv2.putText(img, text, (x, y), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 255, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function recognizes the person in image passed\n",
    "#and draws a rectangle around detected face with name of the \n",
    "#subject\n",
    "def predict(test_img):\n",
    "#make a copy of the image as we don't want to change original image\n",
    "img = test_img.copy()\n",
    "#detect face from the image\n",
    "face, rect = detect_face(img)\n",
    "\n",
    "#predict the image using our face recognizer \n",
    "label= face_recognizer.predict(face)\n",
    "#get name of respective label returned by face recognizer\n",
    "label_text = subjects[label]\n",
    " \n",
    "#draw a rectangle around face detected\n",
    "draw_rectangle(img, rect)\n",
    "#draw name of predicted person\n",
    "draw_text(img, label_text, rect[0], rect[1]-5)\n",
    " \n",
    "return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicting images...\")\n",
    "\n",
    "#load test images\n",
    "test_img1 = cv2.imread(\"test-data/test1.jpg\")\n",
    "test_img2 = cv2.imread(\"test-data/test2.jpg\")\n",
    "\n",
    "#perform a prediction\n",
    "predicted_img1 = predict(test_img1)\n",
    "predicted_img2 = predict(test_img2)\n",
    "print(\"Prediction complete\")\n",
    "\n",
    "#display both images\n",
    "cv2.imshow(subjects[1], predicted_img1)\n",
    "cv2.imshow(subjects[2], predicted_img2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Open  CV YOLO\n",
    "https://www.arunponnusamy.com/yolo-object-detection-opencv-python.html  \n",
    "https://github.com/zhreshold/mxnet-yolo  \n",
    "https://pjreddie.com/darknet/yolo/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "# handle command line arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-i', '--image', required=True,\n",
    "                help = 'path to input image')\n",
    "ap.add_argument('-c', '--config', required=True,\n",
    "                help = 'path to yolo config file')\n",
    "ap.add_argument('-w', '--weights', required=True,\n",
    "                help = 'path to yolo pre-trained weights')\n",
    "ap.add_argument('-cl', '--classes', required=True,\n",
    "                help = 'path to text file containing class names')\n",
    "args = ap.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input image\n",
    "image = cv2.imread(args.image)\n",
    "\n",
    "Width = image.shape[1]\n",
    "Height = image.shape[0]\n",
    "scale = 0.00392\n",
    "\n",
    "# read class names from text file\n",
    "classes = None\n",
    "with open(args.classes, 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# generate different colors for different classes \n",
    "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "\n",
    "# read pre-trained model and config file\n",
    "net = cv2.dnn.readNet(args.weights, args.config)\n",
    "\n",
    "# create input blob \n",
    "blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
    "\n",
    "# set input blob for the network\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output layer and bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the output layer names \n",
    "# in the architecture\n",
    "def get_output_layers(net):\n",
    "    \n",
    "    layer_names = net.getLayerNames()\n",
    "    \n",
    "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "    return output_layers\n",
    "\n",
    "# function to draw bounding box on the detected object with class name\n",
    "def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
    "\n",
    "    label = str(classes[class_id])\n",
    "\n",
    "    color = COLORS[class_id]\n",
    "\n",
    "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
    "\n",
    "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference through the network\n",
    "# and gather predictions from output layers\n",
    "outs = net.forward(get_output_layers(net))\n",
    "\n",
    "# initialization\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "# for each detetion from each output layer \n",
    "# get the confidence, class id, bounding box params\n",
    "# and ignore weak detections (confidence < 0.5)\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * Width)\n",
    "            center_y = int(detection[1] * Height)\n",
    "            w = int(detection[2] * Width)\n",
    "            h = int(detection[3] * Height)\n",
    "            x = center_x - w / 2\n",
    "            y = center_y - h / 2\n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply non-max suppression\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "# go through the detections remaining\n",
    "# after nms and draw bounding box\n",
    "for i in indices:\n",
    "    i = i[0]\n",
    "    box = boxes[i]\n",
    "    x = box[0]\n",
    "    y = box[1]\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    \n",
    "    draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
    "\n",
    "# display output image    \n",
    "cv2.imshow(\"object detection\", image)\n",
    "\n",
    "# wait until any key is pressed\n",
    "cv2.waitKey()\n",
    "    \n",
    " # save output image to disk\n",
    "cv2.imwrite(\"object-detection.jpg\", image)\n",
    "\n",
    "# release resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProjectVirEnv",
   "language": "python",
   "name": "myprojectvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
